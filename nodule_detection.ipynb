{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9CedIOJ5DKo8",
        "UE37pBTFDO_c",
        "EHynfkqTDRaR"
      ],
      "authorship_tag": "ABX9TyMAkrig9J5gwrjVHRPtuKWg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rlsn/COPD_Classification/blob/main/nodule_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "9CedIOJ5DKo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SimpleITK\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "15b9jcTw9fQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import SimpleITK as sitk\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import glob, os\n",
        "\n",
        "datadir=\"datasets/luna16\""
      ],
      "metadata": {
        "id": "O1WwLKa_Mz_d"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_zGdkvKULL4"
      },
      "outputs": [],
      "source": [
        "# download dataset\n",
        "!mkdir -p $datadir\n",
        "!wget -O $datadir/annotations.csv https://zenodo.org/records/3723295/files/annotations.csv?download=1\n",
        "\n",
        "\n",
        "!wget -O $datadir/subset0.zip https://zenodo.org/records/3723295/files/subset0.zip?download=1\n",
        "!unzip $datadir/subset0.zip -d $datadir\n",
        "# for i in range(7):\n",
        "#     !wget -O $datadir/subset$i.zip https://zenodo.org/records/3723295/files/subset$i.zip?download=1\n",
        "#     !unzip $datadir/subset$i.zip -d $datadir\n",
        "# for i in range(7,10):\n",
        "#     !wget -O $datadir/subset$i.zip https://zenodo.org/records/4121926/files/subset$i.zip?download=1\n",
        "#     !unzip $datadir/subset$i.zip -d $datadir"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "UE37pBTFDO_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_image(image_file):\n",
        "    # Read the MetaImage file\n",
        "    image = sitk.ReadImage(image_file, imageIO=\"MetaImageIO\")\n",
        "    image_array = sitk.GetArrayFromImage(image)\n",
        "\n",
        "    # print the image's dimensions\n",
        "    return image_array, np.array(image.GetOrigin()), np.array(image.GetSpacing())\n",
        "\n",
        "def read_csv(fn):\n",
        "    with open(fn,\"r\") as f:\n",
        "        lines = [l.strip().split(\",\") for l in f.readlines()]\n",
        "    return lines\n",
        "\n",
        "def survey_dataset(datadir=\".\"):\n",
        "    data_split = dict()\n",
        "    for i in range(10):\n",
        "        files = glob.glob(f\"{datadir}/subset{i}/*mhd\")\n",
        "        data_split[i]=files\n",
        "    return data_split\n",
        "\n",
        "def add_marker(img, bbox):\n",
        "    low, high = bbox\n",
        "    center = ((low+high)/2).astype(int)\n",
        "    mark = np.zeros_like(img)\n",
        "    new_img = np.copy(img)\n",
        "    value = img.max() if new_img[center[0],center[1]]<(img.max()-img.min())/2 else img.min()\n",
        "    new_img[low[0]:high[0],low[1]]=value\n",
        "    new_img[low[0]:high[0],high[1]]=value\n",
        "    new_img[low[0],low[1]:high[1]]=value\n",
        "    new_img[high[0],low[1]:high[1]]=value\n",
        "    return new_img\n",
        "\n",
        "def convert_loc(coord, origin, space):\n",
        "    displacement = np.array(coord[:3]).astype(float)-origin\n",
        "    loc = np.round(displacement/space)[::-1]\n",
        "    return loc\n",
        "\n",
        "def convert_radius(coord, space):\n",
        "    r = np.round(float(coord[-1])/2/space)[::-1]\n",
        "    return r\n",
        "\n",
        "def convert_bounding_box(coord, origin, space):\n",
        "    center = convert_loc(coord, origin, space)\n",
        "    rad = convert_radius(coord, space)\n",
        "    low = np.round(center-rad)\n",
        "    high = np.round(center+rad)\n",
        "    return low, high\n",
        "\n",
        "def mark_bbox(img, bbox):\n",
        "    low, high = bbox\n",
        "    low=low.astype(int)\n",
        "    high=high.astype(int)\n",
        "\n",
        "    marked_imgs = np.copy(img)\n",
        "    for z in range(low[0],high[0]+1):\n",
        "        marked_imgs[z] = add_marker(img[z],(low[1:],high[1:]))\n",
        "    return marked_imgs\n",
        "\n",
        "def export_as_gif(filename, image_array, frames_per_second=10, rubber_band=False):\n",
        "    images = []\n",
        "    image_array = (image_array-image_array.min())/(image_array.max()-image_array.min())\n",
        "    for arr in image_array:\n",
        "        im = Image.fromarray(np.uint8(arr*255))\n",
        "        images.append(im)\n",
        "    if rubber_band:\n",
        "        images += images[2:-1][::-1]\n",
        "    images[0].save(\n",
        "        filename,\n",
        "        save_all=True,\n",
        "        append_images=images[1:],\n",
        "        duration=1000 // frames_per_second,\n",
        "        loop=0,\n",
        "    )"
      ],
      "metadata": {
        "id": "p1hYDDSkQM6X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "# compute mean and std\n",
        "def compute_stats(dataset):\n",
        "    N = 0\n",
        "    sum = 0\n",
        "    for fn in dataset.filenames:\n",
        "        image,_,_=read_image(fn)\n",
        "        sum += np.sum(image)\n",
        "        N+=np.prod(image.shape)\n",
        "    mean = sum/N\n",
        "    N = 0\n",
        "    sum = 0\n",
        "    for fn in dataset.filenames:\n",
        "        image,_,_=read_image(fn)\n",
        "        sum += np.sum((image-mean)**2)\n",
        "        N+=np.prod(image.shape)\n",
        "    std = np.sqrt(sum/N)\n",
        "    return mean, std\n",
        "\n",
        "def getUID(filename):\n",
        "    return os.path.basename(filename)[:-4]\n",
        "\n",
        "def random_crop_3D(img, crop_size):\n",
        "    size = np.array(img.shape)\n",
        "    high = size-crop_size\n",
        "    start = [np.random.randint(0, high=high[0]),\n",
        "           np.random.randint(0, high=high[1]),\n",
        "           np.random.randint(0, high=high[2])]\n",
        "    return img[start[0]:start[0]+crop_size[0],\n",
        "               start[1]:start[1]+crop_size[1],\n",
        "               start[2]:start[2]+crop_size[2]]\n",
        "\n",
        "def random_crop_around_3D(img, bbox, crop_size, margin=[5,20,20]):\n",
        "    im_size = np.array(img.shape)\n",
        "    blow, bhigh = bbox\n",
        "    blow = blow.astype(int)\n",
        "    bhigh = bhigh.astype(int)\n",
        "    margin = np.array(margin)\n",
        "    low = np.maximum(bhigh+margin-crop_size,0)\n",
        "    high = np.minimum(blow-margin, im_size-crop_size)\n",
        "    offset = [np.random.randint(low[0], high=high[0]),\n",
        "           np.random.randint(low[1], high=high[1]),\n",
        "           np.random.randint(low[2], high=high[2])]\n",
        "    return img[offset[0]:offset[0]+crop_size[0],\n",
        "               offset[1]:offset[1]+crop_size[1],\n",
        "               offset[2]:offset[2]+crop_size[2]], offset\n",
        "\n",
        "def random_flip(img, axis):\n",
        "    if np.random.rand()<0.5:\n",
        "        return np.flip(img, axis=axis)\n",
        "    else:\n",
        "        return img\n",
        "\n",
        "class LUNA16_Dataset(Dataset):\n",
        "    mean = -718.0491779355748\n",
        "    std = 889.6629126452339\n",
        "    \"\"\"\n",
        "    https://luna16.grand-challenge.org/\n",
        "    \"\"\"\n",
        "    def __init__(self, split=None, data_dir=\".\", crop_size=[40,128,128], patch_size=[4,16,16], return_bbox=False):\n",
        "        annotations_csv = read_csv(f\"{data_dir}/annotations.csv\")[1:]\n",
        "        data_subsets = survey_dataset(data_dir)\n",
        "        # to filenames\n",
        "        if split is None:\n",
        "            split = np.arange(10) # all subsets\n",
        "        self.filenames = []\n",
        "        for s in split:\n",
        "            self.filenames+=data_subsets[s]\n",
        "        # annotation to dict\n",
        "        self.annotations = dict([(getUID(k),[]) for k in self.filenames])\n",
        "        for entry in annotations_csv:\n",
        "            self.annotations.setdefault(entry[0], [])\n",
        "            self.annotations[entry[0]]+=[entry[1:]]\n",
        "\n",
        "        self.crop_size = np.array(crop_size)\n",
        "        self.patch_size = np.array(patch_size)\n",
        "\n",
        "        self.return_bbox = return_bbox\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fn = self.filenames[idx]\n",
        "        uid = getUID(fn)\n",
        "        image, origin, space = read_image(fn)\n",
        "        coords = self.annotations[uid]\n",
        "        patch_size_mm = self.patch_size * space[::-1]\n",
        "\n",
        "        result = dict()\n",
        "\n",
        "        if len(coords)>0 and np.random.rand()<1:\n",
        "            # crop with a nodule\n",
        "            target_idx = np.random.randint(len(coords))\n",
        "            coord = coords[target_idx]\n",
        "\n",
        "            bbox = convert_bounding_box(coord, origin, space)\n",
        "            cropped_img, offset = random_crop_around_3D(image, bbox, self.crop_size)\n",
        "            offset_bbox = bbox[0] - offset, bbox[1] - offset\n",
        "            target = np.concatenate([offset_bbox[0]/self.crop_size, offset_bbox[1]/self.crop_size])\n",
        "\n",
        "            result[\"label\"] = torch.tensor(1)\n",
        "            result[\"bbox\"] = torch.tensor(target).to(torch.float32)\n",
        "\n",
        "            # for debugging\n",
        "            if self.return_bbox:\n",
        "                marked_imgs = mark_bbox(cropped_img, offset_bbox)\n",
        "                result[\"bbox_imgs\"]=marked_imgs\n",
        "        else:\n",
        "            # random crop\n",
        "            cropped_img = random_crop_3D(image, self.crop_size)\n",
        "            result[\"label\"] = torch.tensor(0)\n",
        "            result[\"bbox\"] = torch.zeros(6)\n",
        "\n",
        "\n",
        "        # random flip\n",
        "        pixel_values = random_flip(cropped_img, 0)\n",
        "        pixel_values = random_flip(pixel_values, 1)\n",
        "        pixel_values = random_flip(pixel_values, 2)\n",
        "\n",
        "        # normalize\n",
        "        pixel_values = (pixel_values-LUNA16_Dataset.mean)/LUNA16_Dataset.std\n",
        "\n",
        "        # to tensor\n",
        "        pixel_values = torch.tensor(pixel_values.copy()).to(torch.float32)\n",
        "        # add channel dim\n",
        "        pixel_values = pixel_values.unsqueeze(0)\n",
        "        result[\"pixel_values\"]=pixel_values\n",
        "        return result\n",
        "\n",
        "\n",
        "# dataset = LUNA16_Dataset(data_dir=datadir)\n",
        "# re = dataset[56]\n",
        "# image = re[\"pixel_values\"]\n",
        "# export_as_gif(\"ct.gif\",image[0])\n",
        "# if \"bbox_imgs\" in re:\n",
        "#     export_as_gif(\"ct_marked.gif\",re[\"bbox_imgs\"])"
      ],
      "metadata": {
        "id": "4oX_mz2MW0sG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    labels = torch.stack([example[\"label\"] for example in examples])\n",
        "    bbox = torch.stack([example[\"bbox\"] for example in examples])\n",
        "\n",
        "\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels, \"bbox\":bbox}\n",
        "\n",
        "\n",
        "# dataloader = DataLoader(dataset, collate_fn=collate_fn,batch_size=4)\n",
        "# x=next(iter(dataloader))\n",
        "# x[\"pixel_values\"].shape,x[\"labels\"].shape,x[\"bbox\"].shape"
      ],
      "metadata": {
        "id": "neeef3o7iQ00"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "EHynfkqTDRaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTModel, ViTConfig, PreTrainedModel\n",
        "from transformers.utils import ModelOutput\n",
        "from transformers.models.vit.modeling_vit import ViTPooler, ViTEncoder\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "import torch.nn as nn\n",
        "\n",
        "class Vit3DEmbeddings(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        num_patches = int(np.prod(np.array(config.image_size)/np.array(config.patch_size)))\n",
        "        patch_dim = np.prod(config.patch_size)*config.num_channels\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n",
        "        self.position_embeddings = nn.Parameter(torch.randn(1, num_patches + 1, config.hidden_size))\n",
        "        self.projection = nn.Conv3d(config.num_channels, config.hidden_size, kernel_size=config.patch_size, stride=config.patch_size)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.config = config\n",
        "    def forward(self, pixel_values):\n",
        "        batch_size, num_channels, depth, height, width = pixel_values.shape\n",
        "        embeddings = self.projection(pixel_values).flatten(2).transpose(1,2)\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
        "        embeddings = embeddings + self.position_embeddings\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "class VitDet(PreTrainedModel):\n",
        "    def __init__(self, config, add_pooling_layer = True):\n",
        "        super().__init__(config)\n",
        "        self.embeddings = Vit3DEmbeddings(config)\n",
        "\n",
        "        self.encoder = ViTEncoder(config)\n",
        "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.pooler = ViTPooler(config) if add_pooling_layer else None\n",
        "        self.classification_head = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        self.bbox_head = nn.Linear(config.hidden_size, 6)\n",
        "\n",
        "        self.config = config\n",
        "    def forward(self, pixel_values, labels=None, bbox=None):\n",
        "        embeddings = self.embeddings(pixel_values)\n",
        "        encoder_outputs = self.encoder(embeddings)\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        sequence_output = self.layernorm(sequence_output)\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "        logits = self.classification_head(pooled_output)\n",
        "        bbox_pred = self.bbox_head(pooled_output)\n",
        "\n",
        "\n",
        "        if labels is not None and bbox is not None:\n",
        "            loss_bbox_fn = MSELoss(reduction='none')\n",
        "            if self.config.num_labels == 1:\n",
        "                loss_cls_fn = BCEWithLogitsLoss()\n",
        "                loss = loss_cls_fn(logits.view(-1), labels.float())\n",
        "            else:\n",
        "                loss_cls_fn = CrossEntropyLoss()\n",
        "                loss = loss_cls_fn(logits, labels)\n",
        "\n",
        "            mask = labels.unsqueeze(-1).bool()\n",
        "            mse_loss = loss_bbox_fn(bbox_pred, bbox)*mask\n",
        "            loss += mse_loss.mean()\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        return ModelOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            bbox=bbox_pred,\n",
        "            last_hidden_state=sequence_output,\n",
        "            pooler_output=pooled_output,\n",
        "        )"
      ],
      "metadata": {
        "id": "12G2mOZpDUZq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = ViTConfig(hidden_size=384,\n",
        "                   num_hidden_layers=4,\n",
        "                   num_attention_heads=6,\n",
        "                   patch_size=[4,16,16],\n",
        "                   image_size=[40,128,128],\n",
        "                   num_channels=1,\n",
        "                   num_labels=1)\n",
        "\n",
        "model = VitDet(config)"
      ],
      "metadata": {
        "id": "y6fM2enjtyr_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyDataset(Dataset):\n",
        "    def __init__(object):\n",
        "        super().__init__()\n",
        "    def __len__(self):\n",
        "        return 10\n",
        "    def __getitem__(self,idx):\n",
        "        result=dict()\n",
        "        result[\"pixel_values\"]=torch.randn(1,40,128,128)\n",
        "        result[\"label\"]=torch.randint(0,2,[])\n",
        "        result[\"bbox\"]=torch.randn(6)\n",
        "\n",
        "        return result\n",
        "train_dataset = DummyDataset()\n",
        "valid_dataset = DummyDataset()\n",
        "\n",
        "dataloader = DataLoader(train_dataset, collate_fn=collate_fn,batch_size=4)\n",
        "x=next(iter(dataloader))\n",
        "# model(**x)"
      ],
      "metadata": {
        "id": "5oP73ScAw7nN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "QdlJjyVZT3Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def iou_3d(bbox_pred,bbox):\n",
        "    ilow = np.maximum(bbox_pred,bbox)[:,:3]\n",
        "    ihigh = np.minimum(bbox_pred,bbox)[:,3:]\n",
        "    i_sides = np.maximum(ihigh-ilow,0)\n",
        "    i_vol = np.prod(i_sides,-1)\n",
        "    o_vol = np.prod(bbox_pred[:,3:]-bbox_pred[:,:3],-1)+np.prod(bbox[:,3:]-bbox[:,:3],-1)-i_vol\n",
        "    return (i_vol/o_vol).mean()\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, groundtruth = eval_pred\n",
        "    logits = predictions[0]\n",
        "    labels = groundtruth[0]\n",
        "\n",
        "    mask = labels.astype(bool)\n",
        "    bbox_pred = predictions[1][mask]\n",
        "    bbox = groundtruth[1][mask]\n",
        "\n",
        "    preds = (logits>0.5).astype(int)\n",
        "    f1 = f1_score(labels, preds)\n",
        "    if bbox.shape[0]>0:\n",
        "        iou = iou_3d(bbox_pred,bbox)\n",
        "    else:\n",
        "        iou = 1.0\n",
        "    return dict(f1=f1, iou=iou)\n",
        "\n",
        "args = TrainingArguments(\n",
        "        f\"luna-train\",\n",
        "        save_strategy=\"steps\",\n",
        "        evaluation_strategy=\"steps\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=10,\n",
        "        per_device_eval_batch_size=10,\n",
        "        num_train_epochs=300,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=1,\n",
        "        save_steps=1,\n",
        "        save_total_limit=5,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        logging_dir='logs',\n",
        "        label_names=[\"labels\",\"bbox\"],\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXnkU0GuJWo-",
        "outputId": "e7c22b73-fadc-4a0a-ae5e-efe4d84a1809"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.train()"
      ],
      "metadata": {
        "id": "PxVBhpuwNRxD"
      },
      "execution_count": 100,
      "outputs": []
    }
  ]
}